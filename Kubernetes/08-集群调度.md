## 八、集群调度

Scheduler是kubernetes的调度器，主要的任务是把定义的pod分配到集群的节点上。听起来非常简单,但有很多要考虑的问题:

* `公平`: 如何保证每个节点都能被分配资源
* `资源高效利用`: 集群所有资源最大化被使用
* `效率`: 调度的性能要好，能够尽快地对大批量的pod完成调度工作
* `灵活`: 允许用户根据自己的需求控制调度的逻辑

Sheduler是作为单独的程序运行的，启动之后会一直坚挺 API Server, 获取 `PodSpec.NodeName` 为空的pod, 对每个pod都会创建一个binding, 表明该pod应该放到哪个节点上



### 8.1 调度过程

调度分为几个部分:首先是过滤掉不满足条件的节点,这个过程称为`predicate`; 然后对通过的节点按照优先级排序，这个是`priority`; 最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误



Predicate有一系列的算法可以使用:

* `PodFitsResources`: 节点 上剩余的资源是否大于 pod请求的资源
* `PodFitsHost`: 如果pod指定了NodeName,检查节点名称是否和NodeName匹配
* `PodFitsHostPorts`: 节点上已经使用的port是否和pod申请的port冲突
* `PodselectorMatches`: 过滤掉和pod指定的label不匹配的节点
* `NoDiskConflict`: 已经 mount 的 volume 的 pod 指定的 volume 不冲突，除非他们都是只读



如果在predicate过程中没有合适的节点，pod 会一-直在pending 状态,不断重试调度，直到有节点满足条件。经过这个步骤，如果有多个节点满足条件，就继续priorities过程: 按照优先级大小对节点排序优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重(该项的重要性)。这些优先级选项包括:

* `LeastRequestedPriority`: 通过计算CPU和Memory的使用率来决定权重，使用率越低权重越高。换句话说，这个优先级指标倾向于资源使用比例更低的节点
* `BalancedResourceAllocation`: 节点上CPU和Memory使用率越接近,权重越高。这个应该和上面的一起使用，不应该单独使用
* `ImagelocalityPriority`: 倾向于已经有要使用镜像的节点，镜像总大小值越大,权重越高

通过算法对所有的优先级项目和权重进行计算，得出最终的结果



> 自定义调度器

除了kubernetes自带的调度器，你也可以编写自己的调度器。通过`spec:schedulername`参数指定调度器的名宇，可以为pod选择某个调度器进行调度。比如下面的pod选择`my-scheduler`进行调度，而不是默认的`default-scheduler`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotion-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulername: my-scheduler # 指定调度器名称
  containers:
    - name: pod-with-second-anntation-container
      image: hub.yyq.com/library/myapp:v1
```



### 8.2 节点亲和性

#### 8.2.1 Pod 与 Node 亲和性

**pod.spec.nodeAffinity**

* `preferredDuringSchedulingIgnoredDuringExecution`: 软策略
* `requiredDuringSchedulingIgnoredDuringExecution`: 硬策略



`requiredDuringSchedulingIgnoredDuringExecution`: 软策略

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
    - name: with-node-affinity
      image: hub.yyq.com/library/myapp:v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulinglgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                  - k8s-node02
```



`preferredDuringSchedulingIgnoredDuringExecution`: 软策略

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
    - name: with-node-affinity
      image: hub.yyq.com/library/myapp:v1
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: source
                operator: In
                values:
                  - coderGoo
```



**键值运算关系**

* `In`: label 的值在某个列表中
* `NotIn`: label 的值不在某个列表中
* `Gt`: label的值大于某个值
* `Lt`: label的值小于某个值
* `Exists`: 某个label存在
* `DoesNotExist`: 某个label不存在



<!--如果 nodeSelectorTerns 下面有多个选项的话，满足任何-个条件就可以了;如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度POD-->



#### 8.2.2 Pod 与 Pod 亲和性

**pod.spec.affinity.podAffinity/podAntiAffinity**

* `preferredDuringSchedulingIgnoredDuringExecution`: 软策略
* `requiredDuringSchedulingIgnoredDuringExecution`: 硬策略



```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
  labels:
    app: pod-3
spec:
  containers:
    - name: pod-3
      image: hub.yyq.com/library/myapp:v1
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - pod-1
          topologyKey: kubernetes.io/hostname
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityIerm:
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - pod-2
              topologyKey: kubernetes.io/hostname
                      
```



#### 8.2.3 亲和性/反亲和性调度策略比较

| 调度策略        | 匹配标签 | 操作符                                  | 拓扑域支持 | 调度目标                 |
| --------------- | -------- | --------------------------------------- | ---------- | ------------------------ |
| nodeAffinity    | 主机     | In, NotIn, Exists, DoesNotExist, Gt, Lt | 否         | 指定主机                 |
| podAffinity     | POD      | In, Notln, Exists, DoesNotExist         | 是         | POD与指定POD同一拓域     |
| podAnitAffinity | POD      | In, NotIn, Exists, DoesNotExist         | 是         | POD与指定POD不在同拓扑域 |



### 8.3 污点(Taint) 和 容忍(Toleration)

节点亲和性，是pod的一种属性(偏好或硬性要求), 它使*pod被吸引到一类特定的节点。Taint则相反，它使节点能够排斥一类特定的pod

Taint和toleration相互配合, 可以用来避免pod被分配到不合适的节点上。每个节点上都可以应用一个或多个taint, 这表示对于那些不能容忍这些taint的pod,是不会被该节点接受的。如果将toleration应用于pod上，则表示这些pod可以(但不要求)被调度到具有匹配taint的节点上



#### 8.3.1 污点(Taint) 

> 1.污点(Taint)的组成

使用`kubectl taint`命令可以给某个Node节点设置污点，Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行,甚至将Node已经存在的Pod驱逐出去

每个污点的组成如下:

```yaml
key=value:effect
```



每个污点有一个key和value作为污点的标签,中value可以为空，effect 描述污点的作用。当前 tainteffect 支持如下三个选项:

* `NoSchedule`: 表示k8s将不会将Pod调度到具有该污点的Node上
* `PreferNoschedule`: 表示k8s将尽量避免将Pod调度到具有该污点的Node上
* `NoExecute`: 表示k8s将不会将Pod调度到具有该污点的Node. 上,同时会将Node. 上已经存在的Pod驱逐出去



> 2.污点的设置、查看和去除

```shell
# 设置污点
kubectl taint nodes node1 key1=value1:NoSchedule

# 节点说明中，查看 Taints 字段
kubectl describe pod pod-name

# 去除污点
kubectl taint nodes node1 key1:NoSchedule-
```



#### 8.3.2 容忍(Toleration)

设置了污点的Node将根据taint的effect: NoSchedule, PreferNoSchedule, NoExecute 和 Pod 之间产生互斥的关系，Pod 将在一定程度上不会被调度到 Node 上。但我们可以在 Pod 上设置容忍(Toleration), 意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上



pod.spec.tolerations

```yaml
tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
    tolerationSeconds: 3600
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
  - key: "key2"
    operator: "Exists"
    effect: "NoSchedule"
```

* 其中 key, value, effect 要与 Node 上设置的 taint 保持一致
* operator 的值为 Exists 将会忽略 value 值
* tolerationSeconds 用于描述当 Pod 需要被驱逐时可以在 Pod 上继续保留运行的时间



1. 当不指定key值时，表示容忍所有的污点key

   ```yaml
   tolerations:
     - operator: "Exists"
   ```

   

2. 当不指定effect值时，表示容忍所有的污点作用

   ```yaml
   tolerations:
     - key: "key"
       operator: "Exsits"
   ```

   

3. 有多个Master存在时，防止资源浪费，可以如下设置

   ```shell
   kubectl taint nodes Node-Name node-role.kubernetes.io/master=:PreferNoSchedule
   ```

   

### 8.4 固定节点调度

> 1 Pod.spec.nodeName 将Pod直接调度到指定的Node节点上，会跳过Scheduler的调度策略, 该匹配规则是强制匹配

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: myweb
spec:
  replicas: 7
  template:
    metadata:
      labels:
        app: myweb
    spec:
      nodeName: k8s-node01
      containers:
        - name: myweb
          image: hub.yyq.com/library/myapp:v1
          ports:
            - containerPort: 80
```



> 2 Pod.spec.nodeSelector: 通过kubernetes的label-selector机制选择节点，由调度器调度策略匹配label,而后调度Pod到目标节点，该匹配规则属于强制约束

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: myweb
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: myweb
    spec:
      nodeSelector:
        type: backEndNode01
      containers:
        - name: myweb
          image: hub.yyq.com/library/myapp:v1
          ports:
            - containerPort: 80
```

