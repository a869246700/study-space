## 七、存储

### 7.1 ConfigMap

#### 7.1.1 ConfigMap的创建方式

> 方式一 使用目录创建

```shell
$ ls docs/user-guide/configmap/kubectl
game.properties
ui.properties

$ cat /home/configmap/kubectl/game.properties
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30

$ cat /home/configmap/kubectl/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

# 创建configmap
$ kubectl create configMap game-confog --from-file=/home/configmap/kubectl
# 查看configmap的详细信息
$ kubectl get cm -o yaml game-config 
$ kubectl describe configmaps game-config
```

`--from-file`指定在目录下的所有文件都会被用在 **ConfigMap** 里面创建一个键值对。键的名字就是文件名，值就是文件的内容



> 方式二 使用文件创建

只要指定为一个文件就可以从单个文件中创建 **ConfigMap**

```shell
$ kubectl create configmap game-config-2 --from-file=/home/configmap/kubectl/game.properties
$ kubectl get cm -o yaml game-config-2
```



> 方式三 使用字面量创建

使用文字值创建，利用`--from-literal`参数传递配置信息，该参数可以使用多次，格式如下

```shell
$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm
$ kubectl get cm -o yaml special-config
```



#### 7.1.2 Pod中使用ConfigMap

> 一 使用 ConfigMap 来代替环境变量

```yaml
# vim special.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  specail.how: very
  specail.type: charm
```

```yaml
# vim env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: env-config
  namespace: default
data:
  log.level: INFO
```

```yaml
# vim pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: hub.yyq.com/library/myapp:v1
      command: ["/bin/sh", "-c", "env"]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
```

```shell
$ kubectl create -f special.yaml
$ kubectl create -f env.yaml
$ kubectl create -f pod.yaml
$ kubectl logs dapi-test-pod
```



> 二 使用 ConfigMap 设置命令行参数

```yaml
# vim special.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  specail.how: very
  specail.type: charm
```

```yaml
# vim pod2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod2
spec:
  containers:
    - name: test-container
      image: hub.yyq.com/library/myapp:v1
      command: ["/bin/sh", "-c", "echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)"]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: special.type
      envFrom:
        - configMapRef:
            name: env-config
  restartPolicy: Never
```

```shell
$ kubectl create -f special.yaml
$ kubectl create -f pod2.yaml
$ kubectl logs dapi-test-pod2
```



> 三 通过数据卷插件使用 ConfigMap

```yaml
# vim special.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  specail.how: very
  specail.type: charm
```

```yaml
# vim pod3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod3
spec:
  containers:
    - name: test-container
      image: hub.yyq.com/library/myapp:v1
      command: ["/bin/sh", "-c", "cat /etc/config/special.how"]
      volumeMounts:
        - name: config-volume
          mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap: 
        name: special-config
  restartPolicy: Never
```



#### 7.1.3 ConfigMap 热更新

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-config
  namespace: default
data:
  log_level: INFO
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
        - name: my-nginx
          image: hub.yyq.com/library/myapp:v1
          ports:
            - containerPort: 80
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
      volumes:
        - name: config-volume
          configMap:
            name: log-config
```

**修改 ConfigMap**

```shell
$ kubectl edit configmap log-config
```

**修改`log_level`的值为`DEBUG`，等待大概10秒钟时间，再次查看环境变量的值**

```shell
$ kubectl exec -it my-nginx-57d7f8b475-f64nj -- cat /etc/config/log_level
```



> 注意点

**ConfigMap 更新后滚动更新 Pod**

更新ConfigMap目前并不会触发相关Pod的滚动更新.可以通过修改pod annotations的方式强制触发滚动更新

```shell
$ kubectl patch deployment my-nginx --patch '{ "template": { "metadata": { "annotations": { "version/config": "20220102" }}}}'
```



**更新 ConfigMap 后**

* 使用该 ConfigMap 挂载的 Env 不会同步更新
* 使用该 ConfigMap 挂载的 Volume 中的数据需要一端时间（实测大概10秒）才能同步更新



### 7.2 Secret

**Secret解决了密码、token. 密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者PodSpec中。Secret 可以以Volume或者环境变量的方式使用**



Secret有三种类型:

* Service Account: 用来访问Kubernetes API,由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes. io/serviceaccount目录中

* Opaque: base64编码格式的Secret, 用来存储密码、密钥等

* kubernetes.io/dockerconfigjson: 用来存储私有docker registry的认证信息



#### 7.2.1 Service Account

Service Account用来访问Kubernetes API,由Kubernetes自动创建，并且会自动挂载到Pod的`/run/secrets/kubernetes.io/serviceaccount`目录中

```shell
$ kubectl run nginx --image nginx
deployment "nginx" created

$ kubectl get pod

$ kubectl exec nginx-xxxx ls /run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
```



#### 7.2.2 Opaque

> 一、创建说明

**Opaque类型的数据是一个map类型，要求vatue是base64编码格式:**

```shell
$ echo -n "admin" | base64
YWRtaW4=
$ echo -m "1f2d1e2e67df" | base64
MWYyZDFlMmU2N2Rm
```

**secrets.yaml**

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
```



> 二、使用方式

**1. 将 Secret 挂载到 Volume 中**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-test
  labels:
    name: secret-test
spec:
  containers:
    - name: db
      image: hub.yyq.com/library/myapp:v1
      volumeMounts:
        - name: secrets
          mountPath: /etc/secrets
          readOnly: true
  volumes:
    - name: secrets
      secret:
        secretName: mysecret
```

**2. 将 Secret 导出到环境变量中**

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pod-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: pod-deployment
    spec:
      restartPolicy: Never
      containers:
        - name: pod-1
          image: hub.yyq.com/library/myapp:v1
          ports:
            - containerPort: 8
          env:
            - name: TEST_USER
              valueFrom:
                secretKeyRef:
                  name: mysecret
                  key: username
            - name: TEST_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysecret
                  key: password
```



#### 7.2.3 kubernetes.io/dockerconfigjson

使用 Kuberctl 创建 `docker registry` 认证的 secret

```shell
$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret "myregistrykey" created.
```

在创建Pod的时候，通过 `imagePullsecrets` 来引用刚创建的`myregistrykey`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
    - name: foo
      image: hub.yyq.com/test/myapp:v1
  imagePullSecrets:
    - name: myregistrykey
```



### 7.3 Volume

容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现-些问题。首先，当容器崩溃时，kubelet会重启它，但是容器中的文件将丢失--容器以干净的状态 (镜像最初的状态)重新启动。其次,在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 volume 抽象就很好的解决了这些问题



> 背景

Kubernetes中的卷有明确的寿命一一与封装它的 Pod相同。所f以,卷的生命比Pod中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当Pod不再存在时，卷也将不复存在。也许更重要的是, Kubernetes支持多种类型的卷，Pod 可以同时使用任意数量的卷



#### 7.3.1 卷的类型

**Kubernetes 支持以下类型的卷：**

* `awsElasticBlockStore` `azureDisk` `azureFile` `cephfs` `csi` `downwardAPI` `emptyDir`
* `fc` `flocker` `gcePersistentDisk` `gitRepol` `glusterfs` `hostPath` `iscsi` `local` `nfs`
* `persistentVolumeClaim` `projected` `portworxvolume` `quobytel` `rbd` `scaleI0` `secret`
* `storageos` `vsphereVolume`



#### 7.3.2 emptyDir

当Pod被分配给节点时，首先创建 emptyDir 卷,并且只要该Pod在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod 中的容器可以读取和写入emptyDir 卷中的相同文件,尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除Pod时，emptyDir 中的数据将被永久删除

<!--注意:容器崩溃不会从节点中移除pod,因此、emptyDir 、卷中的数据在容器崩溃时是安全的-->

**用法**

* 暂存空间，例如用于基于磁盘的合并排序
* 用作长时间计算崩溃恢复时的检查点
* Web服务器容器提供数据时，保存内容管理器容器提取的文件

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
    - name: test-container
      image: hub.yyq.com/library/myapp:v1
      volumeMounts:
        - name: cache-volume
          mountPath: /cache
    - name: liveness-exec-container
      image: busybox
      imagePullPolicy: IfNotPresent
      command: ["/bin/sh", "-c", "touch /tmp/live; sleep 60; rm -rf /tmp/live; sleep 3600"]
  volumes:
    - name: cache-volume
      emptyDir: {}
```



#### 7.3.3 hostPth

`hostPath`卷将主机节点的文件系统中的文件或目录挂载到集群中



`hostPath`的用途如下:

* 运行需要访问 Docker 内部的容器;使用 `/var/1ib/docker` 的 `hostPath`
* 在容器中运行 cAdvisor; 使用 `/dev/cgroups` 的 `hostPath`



除了所需的 `path` 属性之外，用户还可以为 `hostPath` 卷指定 `type`

| 值                | 行为                                                         |
| ----------------- | ------------------------------------------------------------ |
|                   | 空字符串(默认)用于向后兼容，这意味着在挂载hostPath卷之前不会执行任何检查。 |
| DirectoryOrCreate | 如果在给定的路径上没有任何东西存在，那么将根据需要在那里创建一个空目录， 权限设置为0755，与Kubelet具有相同的组和所有权。 |
| Directory         | 给定的路径下必须存在目录                                     |
| FileOrCreate      | 如果在给定的路径上没有任何东西存在，那么会根据需要创建一个空文件， 权限设置为0644，与Kubelet具有相同的组和所有权。 |
| File              | 给定的路径下必须存在文件                                     |
| Socket            | 给定的路径下必须存在UNIX套接字                               |
| CharDevice        | 给定的路径下必须存在字符设备                                 |
| BlockDevice       | 给定的路径下必须存在块设备                                   |

**使用这种卷类型是请注意,因为:**

* 于每个节点上的文件都不同，具賄相同配置(例如从podTemplate创建的)的pod在不同节点上的行为可能会有所不同
* 当Kubernetes按照计划添加资源感知调度时，将无法考虑hostPath 使用的资源
* 在底层主机上创建的文件或目录只能由root写入。您需要在特权容器中以root身份运行进程,或修改主机上的文件权限以便写入 hostPath卷



```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-hostpath
spec:
  containers:
    - name: test-container
      image: hub.yyq.com/library/myapp:v1
      volumeMounts:
        - name: hostpath-volume
          mountPath: /test-hostpath
  volumes:
    - name: hostpath-volume
      hostPath:
        path: /data
        type: Directory
```



### 7.4 Persistent Volume

#### 7.4.1 概念

> `PersistentVolume` (PV)

是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源-样, PV也是集群中的资源。PV 是Volume之类的卷插件，但具有独立于使用PV的Pod的生命周期。此API对象包含存储实现的细节,即NFS、iSCSI或特定于云供应商的存储系统



> `PersistentVolumeClaim` (PVC)

是用户存储的请求。它与Pod相似。Pod 消耗节点资源，PVC 消耗PV资源。Pod 可以请求特定级别的资源(CPU和内存)。声明可以请求特定的大小和访问模式(例如，可以以读/写-次或只读多次模式挂载)



> 静态pv

集群管理员创建一些PV。它们带有可供群集用户使用的实际存储的细节。它们存在于Kubernetes API中,可用于消费



> 动态

当管理员创建的静态PV都不匹配用户的`PersistentVolumeClaim`时，集群可能会尝试动态地为PVC创建卷。配置基于`StorageClasses`: PVC 必须请求[存储类],并粗管理员必须创建并配置该类才能进行动态创建。声明该类为`""`可以有效地禁用其动态配置

要启用基于存储级别的动态存储配置，集群管理员需要启用API server 上的`DefaultstorageClass`[准入控制器]。例如，通过确保`DefaultstorageClass`位于API server组件的- admission-control标志，使用逗号分隔的有序值列表中，可以完成此操作



> 绑定

master中的控制环路监视新的PVC,寻找匹配的PV (如果可能)， 并将它们绑定在一起。如果为新的PVC动态调配PV,则该环路将始终将该PV绑定到PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦PV和PVC绑定后，PersistentVolumeClaim 绑定是排他性的，不管它们是如何绑定的。PVC跟PV绑定是一对一的映射



#### 7.4.2 持久化卷声明的保护

PVC保护的目的是确保由pod正在使用的PVC不会从系统中移除，因为如果被移除的话可能会导致数据丢失

<!--注意:当pod状态为Pending、 并且pod已经分配给节点或pod为、Running状态时，PVC处于活动状态-->

当启用PVC保护alpha功能时，如果用户删除了一个pod正在使用的PVC,则该PVC不会被立即删除。PVC 的删除将被推迟，直到PVC不再被任何pod使用



#### 7.4.3 持久化卷类型

PersistentVolume类型以插件形式实现。 Kubernetes 目前支持以下插件类型:

* GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel)
* FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS
* Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes
* HostPath VMware Photon Portworx Volumes ScalelO Volumes StorageOS



```yaml
apiVersion: v1
kind: PersistenVolume
metadata:
  name: pv0003
spce:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistenVolumeRecliamPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nsf:
    path: /tmp
    server: 172.17.0.2
```



#### 7.4.4 PV访问模式

`PersistentVolume`可以以资源提供者支持的任何方式挂载到主机E。如下表所示，供应商具有不同的功能，每个PV的访问模式都将被设置为该卷支持的特定模式。例如，NFS可以支持多个读/写客户端，但特定的NFS PV可能以只读方式导出到服务器上。每个PV都有一套自己的用来描述特定功能的访问模式



访问模式(accessModes):

* ReadWriteOnce -- 该卷可以被 `单个节点` 以 `读/写模式` 挂载
* ReadOnlyMany  -- 该卷可以被 `多个节点` 以 `只读模式` 挂载
* ReadWriteMany -- 该卷可以被 `多个节点` 以 `读/写模式` 挂载



在命令行中，访问模式缩写为:

* RWO - ReadWriteOnce
* ROX - ReadOnlyMany
* RWX - ReadWriteMany



<!-- 一个卷- -次只能使用一 种访问模式挂载，即使它支持很多访问模式。例如，GCEPersistentDisk 可以由单个节点作为ReadiwriteOnce 模式挂载，或由多个节点以ReadonlyMany 模式挂载，但不能同时挂载 -->



#### 7.4.5 回收策略

* Retaip (保留) -- 手动回收
* Recycle (回收) - -基本擦除 (rm -rf /thevolume/* )
* Delete (删除) -- 关联的存储资产(例如AWS EBS、 GCE PD、 Azure Disk和OpenStack Cinder卷)将被删除

当前，只有NFS和HostPath支持回收策略。AWS EBS、GCE PD、Azure Disk和Cinder卷支持删除策略



#### 7.4.6 状态

卷可以处于以下的某种状态:

* Available (可用)--- 块空闲资源还没有被任何声明绑定
* Bound (已绑定) --- 卷已经被声明绑定
* Released (已释放) --- 声明被删除，但是资源还未被集群重新声明
* Failed (失败) --- 该卷的自动回收失败

命令行会显示绑定到PV的PVC的名称



#### 7.4.7 持久化演示说明 - NFS

> 1. 安装 NFS 服务器

```shell
# 在nfs主机上安装一下依赖
yum install -y nfs-common nfs-utils rpcbind
mkdir /nfs
chmod 666 /nfs/
chown nfsnobody /nfs/
vim /etc/exports
  /nfs *(rw,no_root_squash,no_all_squash,sync)
  
systemctl start rpcbind
systemctl start nfs

# 在所有的k8s集群节点下，安装依赖
yum install -y nfs-utils rpcbind
showmount -e 192.168.137.100
mount -t nfs 192.168.137.100:/nfs /test/
echo hello coderGoo! > 1.html
umount /test/ # 解除挂载
rm -rf /test/ # 删除目录
```



> 2. 部署PV

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfspv1
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfs
    server: 192.168.137.100
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfspv2
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfs1
    server: 192.168.137.100
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfspv3
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: slow
  nfs:
    path: /nfs2
    server: 192.168.137.100
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfspv4
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfs3
    server: 192.168.137.100
```



> 3. 创建服务并使用PVC

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
    - name: web
      port: 80
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: hub.yyq.com/library/myapp:v1
          ports:
            - name: web
              containerPort: 80
          volumeMounts:
            - name: www
              mountPath: /usr/share/nginx/html
  # 创建pvc
  volumeClaimTemplates:
    - metadata:
        name: www
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "nfs"
        resources:
          requests:
            storage: 1Gi
```



#### 7.4.8 关于 StatefulSet

* 匹配 Pod name (网络标识)的模式为: $(statefulset名称)-$(序号)， 比如上面的示例: w至b_0, web-1，web-2
* StatefulSet 为每个Pod副本创建了-一个DNS域名，这个域名的格式为: $(podname).(headless servername),也就意味着服务间是通过Pod域名来通信而非Pod IP,因为当Pod所在Node发生故障时，Pod 会被飘移到其它Node.上，Pod IP会发生变化，但是Pod域名不会有变化
* StatefulSet使用Headless服务来控制Pod的域名,这个域名的FQDN为: (servicename).(namespace).svc.cluster.local,其中，"cluster.local" 指的是集群的域名
* 根据volumeClaimTemplates, 为每个Pod创建一个pvc, pvc的命名规则匹配模式:(volumeClaimTemplates.name)-(pod_ name), 比如上面的volumeMounts.name=www，Podname=web-[0-2],因此创建出来的PVC是www-web-0. www-web-1. www-web-2
* 删除Pod不会删除其pvc,手动删除pvc将自动释放pv

```shell
ping web-0.nginx # ping $(podname).(headless servername)

dig -t A nginx.default.svc.cluster.local. @10.244.0.44 # dig -t A  (servicename).(namespace).svc.cluster.local @(coredns-IP)
```



**Statefulset的启停顺序:**

* 有序部署: 部署StatefulSet时，如果有多个Pod副本，它们会被顺序地创建(从0到N-1)粗,在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态。
* 有序删除: 当Pod被删除时，它们被终止的顺序是从N-1到0。
* 有序扩展: 当对Pod执行扩展操作时，与部署- 样,它前面的Pod必须都处于Running和Ready状态。

```shell
# master控制台 - 1
$ kubectl get pod -w
# master 控制台 - 2
$ kubectl create -f pod.yaml
$ kubectl delete statefulsets.apps web
```

